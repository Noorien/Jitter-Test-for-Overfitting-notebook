{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "NOORIEN  MISBHA_20191ISE0119.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "2lEF1hVtDc3W"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRtjWeLGDc4I"
      },
      "source": [
        "#Jitter Test for Overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA-DwwGQDc4e"
      },
      "source": [
        "In this notebook I describe some intuition about possible testing of predicion models for overfitting.\n",
        "\n",
        "Notebook contain description of the idea, its application to classification and regression models with continuous features, comparison with cross validation and performance considerations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9kyCNArDc4y"
      },
      "source": [
        "###Overfitting in Classification Models\n",
        "\n",
        "Assume that we have models **M_0** (overfitted) and  **M_1** (non-overfitted) that trained on data **[X, y ]**. \n",
        "\n",
        "For overfitted model boundary between predicted categories is bigger than for non-overfitted model. \n",
        "So if we jitter *X* and look at the accuracy of prediction on train data **M_i(jitter(X, \\sigma)) -> y** it will decrease faster with growing **\\sigma** for overfitted model, since more points will cross boundary between predictions.\n",
        "\n",
        "Therefore accuracy decrease should look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWwLSRjBDc5B"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "x = np.linspace(0, 0.5, 100)\n",
        "\n",
        "plt.plot( x, 0.7 - 0.5*x + 0.3*np.exp(-x*20), label = \"Overfitted model\")\n",
        "plt.plot( x, 0.9 - 0.5*x, label = \"Non-Overfitted model\")\n",
        "plt.plot( x, 0.6 - 0.5*x, label = \"Just Bad model\")\n",
        "\n",
        "axes = plt.gca()\n",
        "axes.set_ylim([0, 1.1])\n",
        "\n",
        "plt.legend(loc=3)\n",
        "plt.suptitle(\"Expected decrease of accuracy in jitter test\")\n",
        "\n",
        "axes.set_xlabel('$\\sigma$')\n",
        "axes.set_ylabel('Accuracy')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb2x2RwMDc5G"
      },
      "source": [
        "Probably area under the plot of **Accuracy(M(jitter(X,\\sigma)))** can be better metric to analyze models than accuracy itself \n",
        "since it also can incorporate information about overfitting.\n",
        "\n",
        "Below you can see examples of different prediction models from \"Supervised learning superstitions cheat sheet\" \n",
        "notebook https://github.com/rcompton/ml_cheat_sheet and plots of accuracy in jitter test for these models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6B6lAYhDc5Y"
      },
      "source": [
        "# Jitter test code\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def jitter(X, scale):\n",
        "    #out = X.copy()\n",
        "    if scale > 0:        \n",
        "        return X + np.random.normal(0, scale, X.shape)\n",
        "    return X\n",
        "\n",
        "def jitter_test(classifier, X, y, metric_FUNC = accuracy_score, sigmas = np.linspace(0, 0.5, 30), averaging_N = 5):\n",
        "    out = []\n",
        "    \n",
        "    for s in sigmas:\n",
        "        averageAccuracy = 0.0\n",
        "        for x in range(averaging_N):\n",
        "            averageAccuracy += metric_FUNC( y, classifier.predict(jitter(X, s)))\n",
        "\n",
        "        out.append( averageAccuracy/averaging_N)\n",
        "\n",
        "    return (out, sigmas, np.trapz(out, sigmas))\n",
        "\n",
        "allJT = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMddVyFbDc5j"
      },
      "source": [
        "\"\"\"\n",
        "Build some datasets that I'll demo the models on\n",
        "\"\"\"\n",
        "\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "#sklearn two moons generator makes lots of these...\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "Xs = []\n",
        "ys = []\n",
        "\n",
        "#low noise, plenty of samples, should be easy\n",
        "X0, y0 = sklearn.datasets.make_moons(n_samples=1000, noise=.05)\n",
        "Xs.append(X0)\n",
        "ys.append(y0)\n",
        "\n",
        "#more noise, plenty of samples\n",
        "X1, y1 = sklearn.datasets.make_moons(n_samples=1000, noise=.3)\n",
        "Xs.append(X1)\n",
        "ys.append(y1)\n",
        "\n",
        "#less noise, few samples\n",
        "X2, y2 = sklearn.datasets.make_moons(n_samples=200, noise=.05)\n",
        "Xs.append(X2)\n",
        "ys.append(y2)\n",
        "\n",
        "#more noise, less samples, should be hard\n",
        "X3, y3 = sklearn.datasets.make_moons(n_samples=200, noise=.3)\n",
        "Xs.append(X3)\n",
        "ys.append(y3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oJNtiQLDc5s"
      },
      "source": [
        "def plotter(model, X, Y, ax, npts=5000):\n",
        "    \"\"\"\n",
        "    Simple way to get a visualization of the decision boundary \n",
        "    by applying the model to randomly-chosen points\n",
        "    could alternately use sklearn's \"decision_function\"\n",
        "    at some point it made sense to bring pandas into this\n",
        "    \"\"\"\n",
        "    xs = []\n",
        "    ys = []\n",
        "    cs = []\n",
        "    for _ in range(npts):\n",
        "        x0spr = max(X[:,0])-min(X[:,0])\n",
        "        x1spr = max(X[:,1])-min(X[:,1])\n",
        "        x = np.random.rand()*x0spr + min(X[:,0])\n",
        "        y = np.random.rand()*x1spr + min(X[:,1])\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "        cs.append(model.predict([x,y]))\n",
        "    ax.scatter(xs,ys,c=list(map(lambda x:'lightgrey' if x==0 else 'black', cs)), alpha=.35)\n",
        "    ax.hold(True)\n",
        "    ax.scatter(X[:,0],X[:,1],\n",
        "                 c=list(map(lambda x:'r' if x else 'lime',Y)), \n",
        "                 linewidth=0,s=25,alpha=1)\n",
        "    ax.set_xlim([min(X[:,0]), max(X[:,0])])\n",
        "    ax.set_ylim([min(X[:,1]), max(X[:,1])])\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1Uuun2pDc53"
      },
      "source": [
        "####LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNyXs0yzDc55"
      },
      "source": [
        "import sklearn.linear_model\n",
        "classifier = sklearn.linear_model.LogisticRegression()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\n",
        "\n",
        "allJT[str(classifier)] = list()\n",
        "\n",
        "i=0\n",
        "for X,y in zip(Xs,ys): \n",
        "    classifier.fit(X,y)\n",
        "    plotter(classifier,X,y,ax=axes[i//2,i%2])\n",
        "    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Y9jZQPDc6h"
      },
      "source": [
        "####DecisionTreeClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23kCH_VaDc6n"
      },
      "source": [
        "import sklearn.tree\n",
        "classifier = sklearn.tree.DecisionTreeClassifier()\n",
        "\n",
        "allJT[str(classifier)] = list()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\n",
        "i=0\n",
        "for X,y in zip(Xs,ys): \n",
        "    classifier.fit(X,y)\n",
        "    plotter(classifier,X,y,ax=axes[i//2,i%2])\n",
        "    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsCjG_feDc6p"
      },
      "source": [
        "####Support Vector Machines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9cupjbrDc9K"
      },
      "source": [
        "import sklearn.svm\n",
        "classifier = sklearn.svm.SVC()\n",
        "\n",
        "allJT[str(classifier)] = list()\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,13))\n",
        "i=0\n",
        "for X,y in zip(Xs,ys): \n",
        "    classifier.fit(X,y)\n",
        "    plotter(classifier,X,y,ax=axes[i//2,i%2])\n",
        "    allJT[str(classifier)].append (jitter_test(classifier, X, y))\n",
        "    i += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJjSlG2BDc9L"
      },
      "source": [
        "####Accuracy in jitter test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG3baI3qDc9M"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(11,10))\n",
        "\n",
        "\n",
        "\n",
        "handlers =[]\n",
        "for c_name in allJT:\n",
        "    for i in range(4): \n",
        "\n",
        "        ax=axes[i//2,i%2]\n",
        "        \n",
        "        ax.set_xlim([0, 0.5])\n",
        "        ax.set_ylim([0.7, 1.1])\n",
        "\n",
        "        accuracy, sigmas, area = allJT[c_name][i]\n",
        "        ax.plot( sigmas, accuracy, label = \"Area: {:.2} \".format(area) + c_name.split(\"(\")[0])\n",
        "        ax.legend()\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmEMNCGCDc9N"
      },
      "source": [
        "####Conclusion\n",
        "\n",
        "DecisionTreeClassifier demonstrate overfitting behavior on high-noise data (rightmost pictures). That can be seen from plot itself and computed area under accuracy curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md7z93VDDc9O"
      },
      "source": [
        "###Overfitting in Regression Models\n",
        "\n",
        "Same approach can be applied to regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d-1To9BDc9P"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Generate sample data for regression\n",
        "np.random.seed(2)\n",
        "N_points = 10\n",
        "X = np.linspace(0, 1, N_points)\n",
        "y = np.random.rand(N_points) + X\n",
        "X = X[:, np.newaxis]\n",
        "\n",
        "allJT = {}\n",
        "\n",
        "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(11,5))\n",
        "\n",
        "ax=axes[0]\n",
        "ax.scatter(X, y)\n",
        "\n",
        "#for i in [1, 5, 9]:\n",
        "for i in [1, 4, 7]:\n",
        "    model = Pipeline([('poly', PolynomialFeatures(degree=i)),\n",
        "                   ('linear', LinearRegression())])\n",
        "\n",
        "    model = model.fit(X, y)\n",
        "    \n",
        "    allJT[i] = jitter_test(model, X, y, lambda x,y: np.exp(-mean_squared_error(x,y)))\n",
        "    \n",
        "    ax.plot(X, model.predict(X), label=\"Degree {}\".format(i))\n",
        "\n",
        "plt.suptitle(\"Polynomial regression\", fontsize=16)\n",
        "ax.legend(loc=2)\n",
        "ax.set_xlabel('$X$')\n",
        "ax.set_ylabel('$y$')\n",
        "\n",
        "\n",
        "\n",
        "ax=axes[1]\n",
        "#ax.set_xlim([0, .4])\n",
        "\n",
        "for i in allJT:\n",
        "    accuracy, sigmas, area = allJT[i]\n",
        "    ax.plot( sigmas, np.exp(accuracy), label = \"{:.3} \".format(area) + \"Degree {}\".format(i) )\n",
        "\n",
        "ax.legend(loc=10)\n",
        "ax.set_xlabel('$\\sigma$')\n",
        "ax.set_ylabel('$exp(-mean_squared_error)$')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "623WanpvDc9c"
      },
      "source": [
        "##Comparison with Cross Validation\n",
        "\n",
        "Let's try to find best model using cross validation and compare results with JTO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rceXRpdTDc9d"
      },
      "source": [
        "from sklearn import cross_validation\n",
        "#more noise, plenty of samples\n",
        "#X1, y1\n",
        "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X1, y1, test_size=0.4, random_state=120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QdtuFZSDc9e"
      },
      "source": [
        "def CV_test( X_train, y_train, depth, averaging, random_state_ = 1):\n",
        "    out =[]\n",
        "    for i in depth:\n",
        "        classifier = sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state = random_state_)\n",
        "        scores = cross_validation.cross_val_score(classifier, X_train, y_train, cv=averaging)\n",
        "        out.append(scores.mean())\n",
        "        \n",
        "    return out\n",
        "\n",
        "def JOT_test(X_train, y_train, depth, averaging, random_state_ = 1):\n",
        "    out =[]\n",
        "    \n",
        "\n",
        "    for i in depth:\n",
        "        classifier = sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state = random_state_)\n",
        "        classifier.fit(X_train, y_train)\n",
        "        jitter_errors, sigmas, score = jitter_test(classifier, X_train, y_train, averaging_N=averaging)\n",
        "        \n",
        "        #plt.plot(sigmas, jitter_errors, alpha=float(i+1)/(max(depth)+1), color=\"blue\")\n",
        "        out.append( score)\n",
        "\n",
        "    #plt.show()\n",
        "    return out\n",
        "\n",
        "test_range = range(1,30)\n",
        "averaging = 50\n",
        "\n",
        "cv_accuracy = CV_test(X_train, y_train, test_range, averaging)\n",
        "jot_score = JOT_test(X_train, y_train, test_range, averaging)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxIf2fs6Dc9f"
      },
      "source": [
        "def plot_scoring(test_range, score, title):\n",
        "    plt.scatter(test_range, score)\n",
        "    plt.title(title)\n",
        "    \n",
        "    best_id = np.argmax(score)\n",
        "    plt.scatter(test_range[best_id], score[best_id], color=\"red\", label = \"Best depth = {}\".format(test_range[best_id]))    \n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    return best_id\n",
        "\n",
        "score_cv_id = plot_scoring(test_range, cv_accuracy, \"Cross validation\")\n",
        "\n",
        "score_JTO_id = plot_scoring(test_range, jot_score, \"Jitter test for overfitting\")\n",
        "\n",
        "# Compute scoring for test set\n",
        "test_score = []\n",
        "for i in test_range:\n",
        "    test_score.append(sklearn.tree.DecisionTreeClassifier(max_depth=i, random_state=1).fit(X_train, y_train).score(X_test, y_test))\n",
        "\n",
        "score_test = plot_scoring(test_range, test_score, \"Test set scoring\")\n",
        "\n",
        "print(\"Scores on test set\")\n",
        "print(\"CV   = {}\\nJTO  = {}\\nTest = {}\".format(test_score[score_cv_id], test_score[score_JTO_id], test_score[score_test]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csXpM9nbDc9g"
      },
      "source": [
        "Let's see how averaged results for JOT and CV looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_qzjXweDc9h"
      },
      "source": [
        "repeat_N = 10\n",
        "test_best_score = 0.\n",
        "jot_best_score = 0.\n",
        "cv_best_score = 0.\n",
        "\n",
        "for i in range(repeat_N):\n",
        "    X_train, X_test, y_train, y_test = cross_validation.train_test_split(X1, y1, test_size=0.4)\n",
        "    \n",
        "    cv_accuracy = CV_test(X_train, y_train, test_range, averaging, None)    \n",
        "    \n",
        "    jot_score = JOT_test(X_train, y_train, test_range, averaging, None)    \n",
        "    \n",
        "    test_score = []\n",
        "    for i in test_range:\n",
        "        test_score.append(sklearn.tree.DecisionTreeClassifier(max_depth=i).fit(X_train, y_train).score(X_test, y_test))\n",
        "        \n",
        "    test_best_score += test_score[np.argmax(test_score)]\n",
        "    jot_best_score += test_score[np.argmax(jot_score)]\n",
        "    cv_best_score += test_score[np.argmax(cv_accuracy)]\n",
        "\n",
        "print(\"Average score:\")\n",
        "print(\"Test = {}\\nJOT  = {}\\nCV   = {}\".format(test_best_score/repeat_N, jot_best_score/repeat_N, cv_best_score/repeat_N))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2aYUoYmDc9i"
      },
      "source": [
        "###Conlusion\n",
        "At least in some cases JOT shows small improvemnet over CV in average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JzTWiyZDc9t"
      },
      "source": [
        "##Performance testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7q0lhsMDc9u"
      },
      "source": [
        "import timeit\n",
        "\n",
        "repeat_N = 10\n",
        "cv_time = timeit.Timer('CV_test(X_train, y_train, test_range, averaging)', \"from __main__ import CV_test, X_train, y_train, test_range, averaging\").timeit(repeat_N)\n",
        "jot_time = timeit.Timer('JOT_test(X_train, y_train, test_range, averaging)', \"from __main__ import JOT_test, X_train, y_train, test_range, averaging\").timeit(repeat_N)\n",
        "\n",
        "print(\"Timing\")\n",
        "print(\"CV  = {} sec.\\nJOT = {} sec.\".format(cv_time, jot_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FfnEEdiDc9x"
      },
      "source": [
        "Jitter test is much slower than cross validation. This could be explained by non-optimal implementation. \n",
        "But in any case, I would expect that it will be faster when model training is very slow. \n",
        "Since in CV case we have to train model on every data subset, then make prediction and find out a score. \n",
        "While for JOT we train model only once and than make multiple predictions."
      ]
    }
  ]
}